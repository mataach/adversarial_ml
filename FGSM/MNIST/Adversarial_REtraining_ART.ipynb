{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "from art.classifiers import PyTorchClassifier\n",
    "from art.utils import load_mnist\n",
    "from art.attacks import FastGradientMethod\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1\n",
    "size = int(50000*percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_mnist()\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1, 3).astype(np.float32)\n",
    "x_test = np.swapaxes(x_test, 1, 3).astype(np.float32)\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(x_train[:size]), torch.Tensor(y_train[:size]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/Users/Matach/Documents/Imperial College London/FYP/saved_models/MNIST/FGSM/FGSM Adversarial/vary_adv_samples_ratio_exp/\")\n",
    "file = \"100%_2.pth\"\n",
    "pretrained_model = os.path.join(path, file)\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 512)\n",
    "        self.fc2 = nn.Linear(512, 200)\n",
    "        self.fc3 = nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train_classifier(cl, opt, x, y):\n",
    "    x.to(device)\n",
    "    y.to(device)\n",
    "    #Reset gradients\n",
    "    opt.zero_grad()\n",
    "    #Train on real data\n",
    "    pred = cl(x)\n",
    "    pred.to(device)\n",
    "    err = F.nll_loss(F.log_softmax(pred, dim=0), y)\n",
    "    err.backward()\n",
    "    #Update optimizer\n",
    "    opt.step()\n",
    "    return err, pred\n",
    "\n",
    "def test_model(cl,test_loader): \n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        cl.eval()\n",
    "        for data, target in test_loader:\n",
    "            output = cl(data.to(device))\n",
    "            pred = output.data.max(1, keepdim=True)[1].to(\"cpu\")\n",
    "            target = np.argmax(target, axis=1)\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        acc_test = float(correct.numpy() / len(test_loader.dataset))\n",
    "        \n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Classifier(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n",
      "Accuracy on benign test examples: 97.11 %\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the model.\n",
    "model = Classifier().to(device)\n",
    "\n",
    "# Load pre-trained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "print(model.eval())\n",
    "\n",
    "# Load loss and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Make a classifier wrapper!\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(min_, max_),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(1, 28, 28),\n",
    "    nb_classes=10,\n",
    ")\n",
    "\n",
    "# Test model\n",
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {} %\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benign -> Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    # Make a classifier wrapper!\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        clip_values=(min_, max_),\n",
    "        loss=criterion,\n",
    "        optimizer=optimizer,\n",
    "        input_shape=(1, 28, 28),\n",
    "        nb_classes=10,\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# ONLY POST-TRAIN WITH THE PERCENTAGE OF ADVERSARIAL IMAGES\n",
    "def adversarial_training(model, device, data, target, epsilon):\n",
    "    dim = data.size()\n",
    "    batch = torch.Tensor()\n",
    "    fgsm = np.random.choice([0, 1], size=dim[0], p=[.4, .6])\n",
    "    pg = list(fgsm).count(1) # necessary to know the shape of the new labels batch\n",
    "    labels = torch.zeros(pg, dtype=torch.long) # labels need to be type long\n",
    "    count = 0\n",
    "    for image, y, is_fgsm in zip(data, target, fgsm):\n",
    "        # VIP Reshape image for model compatibility\n",
    "        image = image.view(1, dim[1], dim[2], dim[3])\n",
    "        image, batch = image.to(device), batch.to(device)\n",
    "        if is_fgsm:\n",
    "            classifier = make_classifier(model)\n",
    "            adv_crafter = FastGradientMethod(classifier, eps=epsilon)\n",
    "            perturbed_image = adv_crafter.generate(x=image.cpu().detach().numpy())\n",
    "            batch = torch.cat((batch, torch.Tensor(perturbed_image).to(device)), dim=0)\n",
    "            labels[count] = y\n",
    "            count += 1\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : Test accuracy:  97.35000000000001 %\n",
      "Epoch  1 : Test accuracy:  97.7 %\n",
      "Epoch  2 : Test accuracy:  97.53 %\n",
      "Epoch  3 : Test accuracy:  97.8 %\n",
      "Performance in the trained model: \n",
      "Test accuracy:  97.8 %\n",
      "FINISHED!!!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "e_losses = []\n",
    "\n",
    "# optimizer\n",
    "cl_opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        target = np.argmax(target, axis=1)  # transform from one-hot to int\n",
    "        new_batch, new_target = adversarial_training(model, device, data, target, epsilon=.3)\n",
    "        # train with new_batch and new target\n",
    "        c_error,c_pred = train_classifier(model,cl_opt, new_batch.to(device), new_target.to(device)) \n",
    "        e_losses.append(c_error.cpu().data.numpy())\n",
    "    acc = test_model(model,test_dataloader)\n",
    "    print(\"Epoch \", e, \": Test accuracy: \", 100*acc, \"%\")\n",
    "\n",
    "print(\"Performance in the trained model: \")\n",
    "acc_test = test_model(model,test_dataloader)\n",
    "print(\"Test accuracy: \", 100*acc_test, \"%\")\n",
    "print(\"FINISHED!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/Users/Matach/Documents/Imperial College London/FYP/saved_models/MNIST/FGSM/FGSM Adversarial/train_benign_adv_exp\")\n",
    "file = \"60%_2.pth\"\n",
    "torch.save(model.state_dict(), os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial -> Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : Test accuracy:  98.41 %\n",
      "Epoch  1 : Test accuracy:  98.61999999999999 %\n",
      "Epoch  2 : Test accuracy:  98.87 %\n",
      "Epoch  3 : Test accuracy:  99.0 %\n",
      "Epoch  4 : Test accuracy:  98.96000000000001 %\n",
      "Performance in the trained model: \n",
      "Test accuracy:  98.96000000000001 %\n",
      "FINISHED!!!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "e_losses = []\n",
    "\n",
    "# optimizer\n",
    "cl_opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        target = np.argmax(target, axis=1)  # transform from one-hot to int\n",
    "        c_error,c_pred = train_classifier(model,cl_opt, data.to(device), target.to(device)) \n",
    "        e_losses.append(c_error.cpu().data.numpy())\n",
    "    acc = test_model(model,test_dataloader)\n",
    "    print(\"Epoch \", e, \": Test accuracy: \", 100*acc, \"%\")\n",
    "\n",
    "print(\"Performance in the trained model: \")\n",
    "acc_test = test_model(model,test_dataloader)\n",
    "print(\"Test accuracy: \", 100*acc_test, \"%\")\n",
    "print(\"FINISHED!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/Users/Matach/Documents/Imperial College London/FYP/saved_models/MNIST/FGSM/FGSM Adversarial/train_adv_benign_exp\")\n",
    "file = \"100%_2.pth\"\n",
    "torch.save(model.state_dict(), os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP-TF-GPU-2.1",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
